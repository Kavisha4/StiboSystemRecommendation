# -*- coding: utf-8 -*-
"""StiboSystemsRecommendation System

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yHbLgXX7cL39JG9feYFXLVssSsXOZK8l
"""



"""# **Apriopri Association Rule Mining**

# Imports
"""

# ----------------------------
# BASE
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import datetime as dt

# ----------------------------
# INSTALL
# !pip install mlxtend

# ----------------------------
# TRANSACTION ENCODER
from mlxtend.preprocessing import TransactionEncoder

# ----------------------------
# APRIORI FUNCTION
from mlxtend.frequent_patterns import apriori, association_rules

# ----------------------------
# ITERTOOLS
import itertools

# ----------------------------
# CONFIGURATION
import warnings
warnings.filterwarnings("ignore")
warnings.simplefilter(action='ignore', category=FutureWarning)

pd.set_option('display.max_columns', None)
pd.options.display.float_format = '{:.2f}'.format

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/GroceryStoreDataSet.csv",names=['products'],header=None)
df
df.shape

"""# Cleaning the data"""

data = list(df["products"].apply(lambda x:x.split(',')))

from mlxtend.preprocessing import TransactionEncoder
te = TransactionEncoder()
te_data = te.fit(data).transform(data)
df = pd.DataFrame(te_data,columns=te.columns_).astype(int)

df

first = pd.DataFrame(df.sum() / df.shape[0], columns = ["Support"]).sort_values("Support", ascending = False)
first

# Elimination by Support Value
first[first.Support >= 0.15]

second = list(itertools.combinations(first.index, 2))
second = [list(i) for i in second]
# Sample of combinations
second[:10]

# Finding support values
value = []
for i in range(0, len(second)):
    temp = df.T.loc[second[i]].sum()
    temp = len(temp[temp == df.T.loc[second[i]].shape[0]]) / df.shape[0]
    value.append(temp)
# Create a data frame
secondIteration = pd.DataFrame(value, columns = ["Support"])
secondIteration["index"] = [tuple(i) for i in second]
secondIteration['length'] = secondIteration['index'].apply(lambda x:len(x))
secondIteration = secondIteration.set_index("index").sort_values("Support", ascending = False)
# Elimination by Support Value
secondIteration = secondIteration[secondIteration.Support > 0.1]
secondIteration



"""# Function for Association Rules"""

def ar_iterations(data, num_iter = 1, support_value = 0.1, iterationIndex = None):

    # Next Iterations
    def ar_calculation(iterationIndex = iterationIndex):
        # Calculation of support value
        value = []
        for i in range(0, len(iterationIndex)):
            result = data.T.loc[iterationIndex[i]].sum()
            result = len(result[result == data.T.loc[iterationIndex[i]].shape[0]]) / data.shape[0]
            value.append(result)
        # Bind results
        result = pd.DataFrame(value, columns = ["Support"])
        result["index"] = [tuple(i) for i in iterationIndex]
        result['length'] = result['index'].apply(lambda x:len(x))
        result = result.set_index("index").sort_values("Support", ascending = False)
        # Elimination by Support Value
        result = result[result.Support > support_value]
        return result

    # First Iteration
    first = pd.DataFrame(df.T.sum(axis = 1) / df.shape[0], columns = ["Support"]).sort_values("Support", ascending = False)
    first = first[first.Support > support_value]
    first["length"] = 1

    if num_iter == 1:
        res = first.copy()

    # Second Iteration
    elif num_iter == 2:

        second = list(itertools.combinations(first.index, 2))
        second = [list(i) for i in second]
        res = ar_calculation(second)

    # All Iterations > 2
    else:
        nth = list(itertools.combinations(set(list(itertools.chain(*iterationIndex))), num_iter))
        nth = [list(i) for i in nth]
        res = ar_calculation(nth)

    return res

iteration1 = ar_iterations(df, num_iter=1, support_value=0.1)
iteration1

iteration2 = ar_iterations(df, num_iter=2, support_value=0.1)
iteration2

iteration3 = ar_iterations(df, num_iter=3, support_value=0.01,
              iterationIndex=iteration2.index)
iteration3

iteration4 = ar_iterations(df, num_iter=4, support_value=0.01,
              iterationIndex=iteration3.index)
iteration4

"""# Association Rules"""

# Apriori
freq_items = apriori(df, min_support = 0.1, use_colnames = True, verbose = 1)
freq_items.sort_values("support", ascending = False)



"""Support value gives us these information:
Head 5

65 percent of 100 purchases are "BREAD"
40 percent of 100 purchases are "COFFEE"
35 percent of 100 purchases are "BISCUIT"
35 percent of 100 purchases are "TEA"
30 percent of 100 purchases are "CORNFLAKES"
"""

# Association Rules & Info
df_ar = association_rules(freq_items, metric = "confidence", min_threshold = 0.5)
df_ar

"""

*   Antecedent support variable tells us probability of antecedent products alone
Consequents support variable tells us probability of consequents products alone
The support value is the value of the two products (Antecedents and Consequents)
Confidence is an indication of how often the rule has been found to be true.
The ratio of the observed support to that expected if X and Y were independent.

"""



df_ar[(df_ar.support > 0.15) & (df_ar.confidence > 0.5)].sort_values("confidence", ascending = False)

def get_recommendations(product, df, association_rules):
    # Filter association rules for the given product
    filtered_rules = association_rules[association_rules['antecedents'] == frozenset({product})]

    if not filtered_rules.empty:
        # Get the consequents (recommended products) from the filtered rules
        recommended_products = list(filtered_rules['consequents'])
        recommended_products = [item for sublist in recommended_products for item in sublist]
        return recommended_products
    else:
        return []

# Example usage
product_to_recommend = "TEA"
recommended_products = get_recommendations(product_to_recommend, df, df_ar)
print(f"Recommended products for {product_to_recommend}: {recommended_products}")

import xml.sax

# Define the SAX handler class to handle XML tags
class ProductHandler(xml.sax.ContentHandler):
    def __init__(self):
        self.current_data = ""  # Keeps track of the current tag being processed
        self.product_id = ""    # To store the product ID
        self.product_name = ""  # To store the product name
        self.products = []      # List to store product details

    def startElement(self, tag, attributes):
        self.current_data = tag
        if tag == "Product":
            # Get product ID attribute from the Product tag
            self.product_id = attributes.get('ID', 'N/A')

    def endElement(self, tag):
        if tag == "Product":
            # After finishing parsing a product, store the details in the list
            self.products.append({"Product ID": self.product_id, "Product Name": self.product_name})
        self.current_data = ""

    def characters(self, content):
        # Extract text content for the product name
        if self.current_data == "Name":
            self.product_name = content.strip()

# Create an XML parser
parser = xml.sax.make_parser()

# Create the SAX content handler instance
handler = ProductHandler()
parser.setContentHandler(handler)

# Path to your XML file (replace with your actual file path)
xml_file_path = "/content/DataSet.xml"

# Parse the XML file
parser.parse(xml_file_path)

# Display parsed product details
for product in handler.products:
    print(f"Product ID: {product['Product ID']}, Product Name: {product['Product Name']}")

from flask import Flask, request, jsonify, send_file
import xml.etree.ElementTree as ET
import pandas as pd
import os
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

app = Flask(__name__)
UPLOAD_FOLDER = "uploads"
OUTPUT_FOLDER = "output"
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(OUTPUT_FOLDER, exist_ok=True)

def parse_xml(file_path):
    tree = ET.parse(file_path)
    root = tree.getroot()

    categories = {}
    products = []

    for product in root.findall(".//Product"):
        product_name = product.get("Name")
        category_id = product.get("ParentID")

        if category_id and product_name:
            if category_id not in categories:
                categories[category_id] = []
            categories[category_id].append(product_name)
            products.append([category_id, product_name])

    return categories, products

def apply_apriori(products):
    df = pd.DataFrame(products, columns=["Category", "Product"])
    grouped = df.groupby("Category")["Product"].apply(list).tolist()

    te = TransactionEncoder()
    te_ary = te.fit(grouped).transform(grouped)
    df_encoded = pd.DataFrame(te_ary, columns=te.columns_)

    frequent_itemsets = apriori(df_encoded, min_support=0.2, use_colnames=True)
    rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.0)
    return rules

def save_to_csv(rules):
    csv_path = os.path.join(OUTPUT_FOLDER, "recommendations.csv")
    rules.to_csv(csv_path, index=False)
    return csv_path

@app.route("/upload", methods=["POST"])
def upload_file():
    if "file" not in request.files:
        return jsonify({"error": "No file uploaded"}), 400

    file = request.files["file"]
    if file.filename == "":
        return jsonify({"error": "No selected file"}), 400

    file_path = os.path.join(UPLOAD_FOLDER, file.filename)
    file.save(file_path)

    categories, products = parse_xml(file_path)
    rules = apply_apriori(products)
    csv_path = save_to_csv(rules)

    return jsonify({"message": "File processed successfully", "csv_path": csv_path})

@app.route("/download", methods=["GET"])
def download_csv():
    csv_path = os.path.join(OUTPUT_FOLDER, "recommendations.csv")
    if os.path.exists(csv_path):
        return send_file(csv_path, as_attachment=True)
    else:
        return jsonify({"error": "No CSV available"}), 404

if __name__ == "__main__":
    app.run(debug=True)

# -*- coding: utf-8 -*-
"""StiboSystemsRecommendation System

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yHbLgXX7cL39JG9feYFXLVssSsXOZK8l
"""



"""# **Apriopri Association Rule Mining**

# Imports
"""

# ----------------------------
# BASE
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import datetime as dt

# ----------------------------
# INSTALL
# !pip install mlxtend

# ----------------------------
# TRANSACTION ENCODER
from mlxtend.preprocessing import TransactionEncoder

# ----------------------------
# APRIORI FUNCTION
from mlxtend.frequent_patterns import apriori, association_rules

# ----------------------------
# ITERTOOLS
import itertools

# ----------------------------
# CONFIGURATION
import warnings
warnings.filterwarnings("ignore")
warnings.simplefilter(action='ignore', category=FutureWarning)

pd.set_option('display.max_columns', None)
pd.options.display.float_format = '{:.2f}'.format

df = pd.read_csv("/content/GroceryStoreDataSet.csv",names=['products'],header=None)
df
df.shape

"""# Cleaning the data"""

data = list(df["products"].apply(lambda x:x.split(',')))

from mlxtend.preprocessing import TransactionEncoder
te = TransactionEncoder()
te_data = te.fit(data).transform(data)
df = pd.DataFrame(te_data,columns=te.columns_).astype(int)

df

first = pd.DataFrame(df.sum() / df.shape[0], columns = ["Support"]).sort_values("Support", ascending = False)
first

# Elimination by Support Value
first[first.Support >= 0.15]

second = list(itertools.combinations(first.index, 2))
second = [list(i) for i in second]
# Sample of combinations
second[:10]

# Finding support values
value = []
for i in range(0, len(second)):
    temp = df.T.loc[second[i]].sum()
    temp = len(temp[temp == df.T.loc[second[i]].shape[0]]) / df.shape[0]
    value.append(temp)
# Create a data frame
secondIteration = pd.DataFrame(value, columns = ["Support"])
secondIteration["index"] = [tuple(i) for i in second]
secondIteration['length'] = secondIteration['index'].apply(lambda x:len(x))
secondIteration = secondIteration.set_index("index").sort_values("Support", ascending = False)
# Elimination by Support Value
secondIteration = secondIteration[secondIteration.Support > 0.1]
secondIteration



"""# Function for Association Rules"""

def ar_iterations(data, num_iter = 1, support_value = 0.1, iterationIndex = None):

    # Next Iterations
    def ar_calculation(iterationIndex = iterationIndex):
        # Calculation of support value
        value = []
        for i in range(0, len(iterationIndex)):
            result = data.T.loc[iterationIndex[i]].sum()
            result = len(result[result == data.T.loc[iterationIndex[i]].shape[0]]) / data.shape[0]
            value.append(result)
        # Bind results
        result = pd.DataFrame(value, columns = ["Support"])
        result["index"] = [tuple(i) for i in iterationIndex]
        result['length'] = result['index'].apply(lambda x:len(x))
        result = result.set_index("index").sort_values("Support", ascending = False)
        # Elimination by Support Value
        result = result[result.Support > support_value]
        return result

    # First Iteration
    first = pd.DataFrame(df.T.sum(axis = 1) / df.shape[0], columns = ["Support"]).sort_values("Support", ascending = False)
    first = first[first.Support > support_value]
    first["length"] = 1

    if num_iter == 1:
        res = first.copy()

    # Second Iteration
    elif num_iter == 2:

        second = list(itertools.combinations(first.index, 2))
        second = [list(i) for i in second]
        res = ar_calculation(second)

    # All Iterations > 2
    else:
        nth = list(itertools.combinations(set(list(itertools.chain(*iterationIndex))), num_iter))
        nth = [list(i) for i in nth]
        res = ar_calculation(nth)

    return res

iteration1 = ar_iterations(df, num_iter=1, support_value=0.1)
iteration1

iteration2 = ar_iterations(df, num_iter=2, support_value=0.1)
iteration2

iteration3 = ar_iterations(df, num_iter=3, support_value=0.01,
              iterationIndex=iteration2.index)
iteration3

iteration4 = ar_iterations(df, num_iter=4, support_value=0.01,
              iterationIndex=iteration3.index)
iteration4

"""# Association Rules"""

# Apriori
freq_items = apriori(df, min_support = 0.1, use_colnames = True, verbose = 1)
freq_items.sort_values("support", ascending = False)



"""Support value gives us these information:
Head 5

65 percent of 100 purchases are "BREAD"
40 percent of 100 purchases are "COFFEE"
35 percent of 100 purchases are "BISCUIT"
35 percent of 100 purchases are "TEA"
30 percent of 100 purchases are "CORNFLAKES"
"""

# Association Rules & Info
df_ar = association_rules(freq_items, metric = "confidence", min_threshold = 0.5)
df_ar

"""

*   Antecedent support variable tells us probability of antecedent products alone
Consequents support variable tells us probability of consequents products alone
The support value is the value of the two products (Antecedents and Consequents)
Confidence is an indication of how often the rule has been found to be true.
The ratio of the observed support to that expected if X and Y were independent.

"""



df_ar[(df_ar.support > 0.15) & (df_ar.confidence > 0.5)].sort_values("confidence", ascending = False)

def get_associated_products(product, df_ar, min_confidence=0.5, min_support=0.1):
    """Returns products frequently bought with the given product."""
    result = df_ar[
        ((df_ar['antecedents'].apply(lambda x: product in x)) |
         (df_ar['consequents'].apply(lambda x: product in x))) &
        (df_ar['confidence'] >= min_confidence) &
        (df_ar['support'] >= min_support)
    ].sort_values("confidence", ascending=False)

    return result[['antecedents', 'consequents', 'support', 'confidence', 'lift']]

# Example usage
get_associated_products("TEA", df_ar)